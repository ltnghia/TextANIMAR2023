{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORKDIR = \"/home/dtle/hnttruc-local/SHREC2023\"\n",
    "WANDB_KEY = \"dd131d87e8cb0f25dcc8bdd961a5519012bb8483\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dtle/hnttruc-local/SHREC2023\n"
     ]
    }
   ],
   "source": [
    "%cd $WORKDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "!wandb login --relogin $WANDB_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation\n",
    "Download the dataset [here](https://drive.google.com/drive/folders/1PotQ4wmSRDcWwoW6pfURv4dmfKRDoUDn) and splitting it into train and validation set using the script in `tools/dataset_split.py`.\n",
    "\n",
    "Suggested folder structure:\n",
    "```\n",
    "dataset/\n",
    "‚îú‚îÄ‚îÄ csv/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ References.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ TextQuery_GT_Train.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ TextQuery_GT_Train_split.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ TextQuery_GT_Val_split.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ TextQuery_Test.csv\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ TextQuery_Train.csv\n",
    "‚îî‚îÄ‚îÄ PC_OBJ/\n",
    "    ‚îú‚îÄ‚îÄ 0016afdafa241a8b.obj\n",
    "    ‚îú‚îÄ‚îÄ 00b174d953f27354.obj\n",
    "    ‚îú‚îÄ‚îÄ ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dtle/hnttruc-local/SHREC2023\n",
      "Processing /home/dtle/hnttruc-local/SHREC2023\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: shrec2023\n",
      "  Building wheel for shrec2023 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for shrec2023: filename=shrec2023-0.0.1-py3-none-any.whl size=26907 sha256=254c27f7d2479092fa5fa92a4907f7de61395fc6d7da9eafff33f9f09cbe8069\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-nbpywecg/wheels/c1/fd/9c/e14bd4c32e6a51cbba9c56d65aeb036bbe7fecee2e45a38add\n",
      "Successfully built shrec2023\n",
      "Installing collected packages: shrec2023\n",
      "  Attempting uninstall: shrec2023\n",
      "    Found existing installation: shrec2023 0.0.1\n",
      "    Uninstalling shrec2023-0.0.1:\n",
      "      Successfully uninstalled shrec2023-0.0.1\n",
      "Successfully installed shrec2023-0.0.1\n"
     ]
    }
   ],
   "source": [
    "%cd $WORKDIR\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dtle/hnttruc-local/SHREC2023\n",
      "2023-07-14 00:19:16.076878: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-14 00:19:16.251566: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-14 00:19:16.975467: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib\n",
      "2023-07-14 00:19:16.975530: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib\n",
      "2023-07-14 00:19:16.975537: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Overriding configurating\n",
      "Global seed set to 1337\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mminhkhoi1026\u001b[0m (\u001b[33mhcmus-polars\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./runs/wandb/run-20230714_001922-l490ihla\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpoint-cloud-20230714-001920\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/hcmus-polars/hcmus-shrec23-textANIMAR\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/hcmus-polars/hcmus-shrec23-textANIMAR/runs/l490ihla\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "/home/dtle/miniconda3/envs/fsd/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:286: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Global seed set to 1337\n",
      "initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "2023-07-14 00:19:30.018149: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-14 00:19:30.187467: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-14 00:19:30.902617: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib\n",
      "2023-07-14 00:19:30.902692: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib\n",
      "2023-07-14 00:19:30.902700: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Overriding configurating\n",
      "Global seed set to 1337\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Global seed set to 1337\n",
      "initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name              | Type             | Params\n",
      "-------------------------------------------------------\n",
      "0 | pc_extractor      | CurveNet         | 2.1 M \n",
      "1 | lang_extractor    | LangExtractor    | 109 M \n",
      "2 | lang_encoder      | MLP              | 109 M \n",
      "3 | pc_encoder        | MLP              | 4.8 M \n",
      "4 | constrastive_loss | NTXentLoss       | 0     \n",
      "5 | xbm               | CrossBatchMemory | 0     \n",
      "-------------------------------------------------------\n",
      "5.2 M     Trainable params\n",
      "109 M     Non-trainable params\n",
      "114 M     Total params\n",
      "458.744   Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/dtle/miniconda3/envs/fsd/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Validation sanity check:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 1/2 [00:02<00:02,  2.80s/it]/home/dtle/miniconda3/envs/fsd/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:59: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "Global seed set to 1337                                                         \n",
      "Global seed set to 1337\n",
      "/home/dtle/miniconda3/envs/fsd/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  77%|‚ñä| 10/13 [00:09<00:02,  1.01it/s, loss=5.27, v_num=ihla, train_los/home/dtle/miniconda3/envs/fsd/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:59: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 12. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "Epoch 0:  85%|‚ñä| 11/13 [00:10<00:01,  1.04it/s, loss=5.07, v_num=ihla, train_los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|‚ñà| 13/13 [00:12<00:00,  1.08it/s, loss=5.07, v_num=ihla, train_los\u001b[A\n",
      "Epoch 0: 100%|‚ñà| 13/13 [00:12<00:00,  1.05it/s, loss=5.07, v_num=ihla, train_los\u001b[A\n",
      "Epoch 0: 100%|‚ñà| 13/13 [00:12<00:00,  1.05it/s, loss=5.07, v_num=ihla, train_losEpoch 0, global step 10: NN reached 0.05000 (best 0.05000), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=0-NN=0.0500-mAP=0.2043-train_loss=4.9666-val_loss=3.1592.ckpt\" as top 3\n",
      "Epoch 1:  92%|‚ñâ| 12/13 [00:09<00:00,  1.20it/s, loss=3.49, v_num=ihla, train_los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.54s/it]\u001b[A\n",
      "Epoch 1: 100%|‚ñà| 13/13 [00:11<00:00,  1.10it/s, loss=3.49, v_num=ihla, train_los\u001b[A\n",
      "Epoch 1: 100%|‚ñà| 13/13 [00:11<00:00,  1.10it/s, loss=3.49, v_num=ihla, train_losEpoch 1, global step 21: NN reached 0.05000 (best 0.05000), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=1-NN=0.0500-mAP=0.1914-train_loss=3.4210-val_loss=3.1265.ckpt\" as top 3\n",
      "Epoch 2:  92%|‚ñâ| 12/13 [00:10<00:00,  1.17it/s, loss=3.39, v_num=ihla, train_los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.52s/it]\u001b[A\n",
      "Epoch 2: 100%|‚ñà| 13/13 [00:12<00:00,  1.07it/s, loss=3.39, v_num=ihla, train_los\u001b[A\n",
      "Epoch 2: 100%|‚ñà| 13/13 [00:12<00:00,  1.07it/s, loss=3.39, v_num=ihla, train_losEpoch 2, global step 32: NN reached 0.00000 (best 0.05000), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=2-NN=0.0000-mAP=0.2115-train_loss=3.3946-val_loss=3.1164.ckpt\" as top 3\n",
      "Epoch 3:  92%|‚ñâ| 12/13 [00:11<00:00,  1.08it/s, loss=3.38, v_num=ihla, train_los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.62s/it]\u001b[A\n",
      "Epoch 3: 100%|‚ñà| 13/13 [00:13<00:00,  1.00s/it, loss=3.38, v_num=ihla, train_los\u001b[A\n",
      "Epoch 3: 100%|‚ñà| 13/13 [00:13<00:00,  1.00s/it, loss=3.38, v_num=ihla, train_losEpoch 3, global step 43: NN reached 0.07500 (best 0.07500), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=3-NN=0.0750-mAP=0.2241-train_loss=3.3880-val_loss=3.1143.ckpt\" as top 3\n",
      "Epoch 4:  92%|‚ñâ| 12/13 [00:10<00:00,  1.16it/s, loss=3.38, v_num=ihla, train_los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.52s/it]\u001b[A\n",
      "Epoch 4: 100%|‚ñà| 13/13 [00:12<00:00,  1.06it/s, loss=3.38, v_num=ihla, train_los\u001b[A\n",
      "Epoch 4: 100%|‚ñà| 13/13 [00:12<00:00,  1.06it/s, loss=3.38, v_num=ihla, train_losEpoch 4, global step 54: NN was not in top 3\n",
      "Epoch 5:  92%|‚ñâ| 12/13 [00:10<00:00,  1.16it/s, loss=3.39, v_num=ihla, train_los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.52s/it]\u001b[A\n",
      "Epoch 5: 100%|‚ñà| 13/13 [00:12<00:00,  1.06it/s, loss=3.39, v_num=ihla, train_los\u001b[A\n",
      "Epoch 5: 100%|‚ñà| 13/13 [00:12<00:00,  1.06it/s, loss=3.39, v_num=ihla, train_losEpoch 5, global step 65: NN was not in top 3\n",
      "Epoch 6:  92%|‚ñâ| 12/13 [00:11<00:00,  1.06it/s, loss=3.39, v_num=ihla, train_los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.53s/it]\u001b[A\n",
      "Epoch 6: 100%|‚ñà| 13/13 [00:13<00:00,  1.02s/it, loss=3.39, v_num=ihla, train_los\u001b[A\n",
      "Epoch 6: 100%|‚ñà| 13/13 [00:13<00:00,  1.02s/it, loss=3.39, v_num=ihla, train_losEpoch 6, global step 76: NN was not in top 3\n",
      "Epoch 7:  92%|‚ñâ| 12/13 [00:10<00:00,  1.15it/s, loss=3.39, v_num=ihla, train_los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.55s/it]\u001b[A\n",
      "Epoch 7: 100%|‚ñà| 13/13 [00:12<00:00,  1.06it/s, loss=3.39, v_num=ihla, train_los\u001b[A\n",
      "Epoch 7: 100%|‚ñà| 13/13 [00:12<00:00,  1.06it/s, loss=3.39, v_num=ihla, train_losEpoch 7, global step 87: NN reached 0.17500 (best 0.17500), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=7-NN=0.1750-mAP=0.3210-train_loss=3.3990-val_loss=3.1153.ckpt\" as top 3\n",
      "Epoch 8:  92%|‚ñâ| 12/13 [00:10<00:00,  1.18it/s, loss=3.38, v_num=ihla, train_los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.53s/it]\u001b[A\n",
      "Epoch 8: 100%|‚ñà| 13/13 [00:12<00:00,  1.07it/s, loss=3.38, v_num=ihla, train_los\u001b[A\n",
      "Epoch 8: 100%|‚ñà| 13/13 [00:12<00:00,  1.07it/s, loss=3.38, v_num=ihla, train_losEpoch 8, global step 98: NN reached 0.07500 (best 0.17500), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=8-NN=0.0750-mAP=0.2609-train_loss=3.3897-val_loss=3.1145.ckpt\" as top 3\n",
      "Epoch 9:  92%|‚ñâ| 12/13 [00:10<00:00,  1.13it/s, loss=3.38, v_num=ihla, train_los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.53s/it]\u001b[A\n",
      "Epoch 9: 100%|‚ñà| 13/13 [00:12<00:00,  1.03it/s, loss=3.38, v_num=ihla, train_los\u001b[A\n",
      "Epoch 9: 100%|‚ñà| 13/13 [00:12<00:00,  1.03it/s, loss=3.38, v_num=ihla, train_losEpoch 9, global step 109: NN reached 0.12500 (best 0.17500), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=9-NN=0.1250-mAP=0.2899-train_loss=3.3877-val_loss=3.1099.ckpt\" as top 3\n",
      "Epoch 10:  92%|‚ñâ| 12/13 [00:09<00:00,  1.25it/s, loss=3.39, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.51s/it]\u001b[A\n",
      "Epoch 10: 100%|‚ñà| 13/13 [00:11<00:00,  1.12it/s, loss=3.39, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 10: 100%|‚ñà| 13/13 [00:11<00:00,  1.12it/s, loss=3.39, v_num=ihla, train_loEpoch 10, global step 120: NN reached 0.20000 (best 0.20000), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=10-NN=0.2000-mAP=0.3375-train_loss=3.3886-val_loss=3.1102.ckpt\" as top 3\n",
      "Epoch 11:  92%|‚ñâ| 12/13 [00:10<00:00,  1.20it/s, loss=3.39, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.60s/it]\u001b[A\n",
      "Epoch 11: 100%|‚ñà| 13/13 [00:11<00:00,  1.08it/s, loss=3.39, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 11: 100%|‚ñà| 13/13 [00:11<00:00,  1.08it/s, loss=3.39, v_num=ihla, train_loEpoch 11, global step 131: NN reached 0.15000 (best 0.20000), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=11-NN=0.1500-mAP=0.2998-train_loss=3.3921-val_loss=3.1098.ckpt\" as top 3\n",
      "Epoch 12:  92%|‚ñâ| 12/13 [00:09<00:00,  1.31it/s, loss=3.39, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.62s/it]\u001b[A\n",
      "Epoch 12: 100%|‚ñà| 13/13 [00:11<00:00,  1.17it/s, loss=3.39, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 12: 100%|‚ñà| 13/13 [00:11<00:00,  1.17it/s, loss=3.39, v_num=ihla, train_loEpoch 12, global step 142: NN was not in top 3\n",
      "Epoch 13:  92%|‚ñâ| 12/13 [00:10<00:00,  1.15it/s, loss=3.39, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.60s/it]\u001b[A\n",
      "Epoch 13: 100%|‚ñà| 13/13 [00:12<00:00,  1.05it/s, loss=3.39, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 13: 100%|‚ñà| 13/13 [00:12<00:00,  1.05it/s, loss=3.39, v_num=ihla, train_loEpoch 13, global step 153: NN was not in top 3\n",
      "Epoch 14:  92%|‚ñâ| 12/13 [00:10<00:00,  1.17it/s, loss=3.39, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.56s/it]\u001b[A\n",
      "Epoch 14: 100%|‚ñà| 13/13 [00:12<00:00,  1.07it/s, loss=3.39, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 14: 100%|‚ñà| 13/13 [00:12<00:00,  1.07it/s, loss=3.39, v_num=ihla, train_loEpoch 14, global step 164: NN was not in top 3\n",
      "Epoch 15:  92%|‚ñâ| 12/13 [00:10<00:00,  1.16it/s, loss=3.38, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.50s/it]\u001b[A\n",
      "Epoch 15: 100%|‚ñà| 13/13 [00:12<00:00,  1.06it/s, loss=3.38, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 15: 100%|‚ñà| 13/13 [00:12<00:00,  1.06it/s, loss=3.38, v_num=ihla, train_loEpoch 15, global step 175: NN was not in top 3\n",
      "Epoch 16:  92%|‚ñâ| 12/13 [00:10<00:00,  1.18it/s, loss=3.39, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.56s/it]\u001b[A\n",
      "Epoch 16: 100%|‚ñà| 13/13 [00:12<00:00,  1.07it/s, loss=3.39, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 16: 100%|‚ñà| 13/13 [00:12<00:00,  1.07it/s, loss=3.39, v_num=ihla, train_loEpoch 16, global step 186: NN was not in top 3\n",
      "Epoch 17:  92%|‚ñâ| 12/13 [00:10<00:00,  1.16it/s, loss=3.39, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.58s/it]\u001b[A\n",
      "Epoch 17: 100%|‚ñà| 13/13 [00:12<00:00,  1.05it/s, loss=3.39, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 17: 100%|‚ñà| 13/13 [00:12<00:00,  1.05it/s, loss=3.39, v_num=ihla, train_loEpoch 17, global step 197: NN was not in top 3\n",
      "Epoch 18:  92%|‚ñâ| 12/13 [00:10<00:00,  1.19it/s, loss=3.39, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.52s/it]\u001b[A\n",
      "Epoch 18: 100%|‚ñà| 13/13 [00:11<00:00,  1.09it/s, loss=3.39, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 18: 100%|‚ñà| 13/13 [00:11<00:00,  1.09it/s, loss=3.39, v_num=ihla, train_loEpoch 18, global step 208: NN was not in top 3\n",
      "Epoch 19:  92%|‚ñâ| 12/13 [00:10<00:00,  1.10it/s, loss=3.38, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.60s/it]\u001b[A\n",
      "Epoch 19: 100%|‚ñà| 13/13 [00:12<00:00,  1.01it/s, loss=3.38, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 19: 100%|‚ñà| 13/13 [00:12<00:00,  1.00it/s, loss=3.38, v_num=ihla, train_loEpoch 19, global step 219: NN was not in top 3\n",
      "Epoch 20:  92%|‚ñâ| 12/13 [00:10<00:00,  1.10it/s, loss=3.35, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.53s/it]\u001b[A\n",
      "Epoch 20: 100%|‚ñà| 13/13 [00:12<00:00,  1.01it/s, loss=3.35, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 20: 100%|‚ñà| 13/13 [00:12<00:00,  1.01it/s, loss=3.35, v_num=ihla, train_loEpoch 20, global step 230: NN was not in top 3\n",
      "Epoch 21:  92%|‚ñâ| 12/13 [00:11<00:00,  1.06it/s, loss=3.3, v_num=ihla, train_los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.53s/it]\u001b[A\n",
      "Epoch 21: 100%|‚ñà| 13/13 [00:13<00:00,  1.01s/it, loss=3.3, v_num=ihla, train_los\u001b[A\n",
      "Epoch 21: 100%|‚ñà| 13/13 [00:13<00:00,  1.02s/it, loss=3.3, v_num=ihla, train_losEpoch 21, global step 241: NN was not in top 3\n",
      "Epoch 22:  92%|‚ñâ| 12/13 [00:10<00:00,  1.10it/s, loss=3.19, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.54s/it]\u001b[A\n",
      "Epoch 22: 100%|‚ñà| 13/13 [00:12<00:00,  1.01it/s, loss=3.19, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 22: 100%|‚ñà| 13/13 [00:12<00:00,  1.01it/s, loss=3.19, v_num=ihla, train_loEpoch 22, global step 252: NN was not in top 3\n",
      "Epoch 23:  92%|‚ñâ| 12/13 [00:10<00:00,  1.10it/s, loss=3.01, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.61s/it]\u001b[A\n",
      "Epoch 23: 100%|‚ñà| 13/13 [00:12<00:00,  1.00it/s, loss=3.01, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 23: 100%|‚ñà| 13/13 [00:12<00:00,  1.00it/s, loss=3.01, v_num=ihla, train_loEpoch 23, global step 263: NN was not in top 3\n",
      "Epoch 24:  92%|‚ñâ| 12/13 [00:10<00:00,  1.14it/s, loss=2.96, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.54s/it]\u001b[A\n",
      "Epoch 24: 100%|‚ñà| 13/13 [00:12<00:00,  1.04it/s, loss=2.96, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 24: 100%|‚ñà| 13/13 [00:12<00:00,  1.04it/s, loss=2.96, v_num=ihla, train_loEpoch 24, global step 274: NN reached 0.15000 (best 0.20000), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=24-NN=0.1500-mAP=0.3107-train_loss=2.9897-val_loss=2.8762.ckpt\" as top 3\n",
      "Epoch 25:  92%|‚ñâ| 12/13 [00:10<00:00,  1.19it/s, loss=2.81, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.57s/it]\u001b[A\n",
      "Epoch 25: 100%|‚ñà| 13/13 [00:12<00:00,  1.08it/s, loss=2.81, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 25: 100%|‚ñà| 13/13 [00:12<00:00,  1.08it/s, loss=2.81, v_num=ihla, train_loEpoch 25, global step 285: NN was not in top 3\n",
      "Epoch 26:  92%|‚ñâ| 12/13 [00:10<00:00,  1.09it/s, loss=2.79, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.57s/it]\u001b[A\n",
      "Epoch 26: 100%|‚ñà| 13/13 [00:12<00:00,  1.00it/s, loss=2.79, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 26: 100%|‚ñà| 13/13 [00:12<00:00,  1.00it/s, loss=2.79, v_num=ihla, train_loEpoch 26, global step 296: NN was not in top 3\n",
      "Epoch 27:  92%|‚ñâ| 12/13 [00:11<00:00,  1.09it/s, loss=2.86, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.58s/it]\u001b[A\n",
      "Epoch 27: 100%|‚ñà| 13/13 [00:13<00:00,  1.00s/it, loss=2.86, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 27: 100%|‚ñà| 13/13 [00:13<00:00,  1.00s/it, loss=2.86, v_num=ihla, train_loEpoch 27, global step 307: NN was not in top 3\n",
      "Epoch 28:  92%|‚ñâ| 12/13 [00:10<00:00,  1.18it/s, loss=2.81, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.59s/it]\u001b[A\n",
      "Epoch 28: 100%|‚ñà| 13/13 [00:12<00:00,  1.07it/s, loss=2.81, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 28: 100%|‚ñà| 13/13 [00:12<00:00,  1.07it/s, loss=2.81, v_num=ihla, train_loEpoch 28, global step 318: NN reached 0.32500 (best 0.32500), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=28-NN=0.3250-mAP=0.4999-train_loss=2.7150-val_loss=2.3853.ckpt\" as top 3\n",
      "Epoch 29:  92%|‚ñâ| 12/13 [00:09<00:00,  1.25it/s, loss=2.65, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.55s/it]\u001b[A\n",
      "Epoch 29: 100%|‚ñà| 13/13 [00:11<00:00,  1.13it/s, loss=2.65, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 29: 100%|‚ñà| 13/13 [00:11<00:00,  1.13it/s, loss=2.65, v_num=ihla, train_loEpoch 29, global step 329: NN was not in top 3\n",
      "Epoch 30:  92%|‚ñâ| 12/13 [00:10<00:00,  1.18it/s, loss=2.63, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.72s/it]\u001b[A\n",
      "Epoch 30: 100%|‚ñà| 13/13 [00:12<00:00,  1.06it/s, loss=2.63, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 30: 100%|‚ñà| 13/13 [00:12<00:00,  1.06it/s, loss=2.63, v_num=ihla, train_loEpoch 30, global step 340: NN reached 0.22500 (best 0.32500), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=30-NN=0.2250-mAP=0.4186-train_loss=2.6650-val_loss=2.2022.ckpt\" as top 3\n",
      "Epoch 31:  92%|‚ñâ| 12/13 [00:10<00:00,  1.09it/s, loss=2.52, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.52s/it]\u001b[A\n",
      "Epoch 31: 100%|‚ñà| 13/13 [00:12<00:00,  1.01it/s, loss=2.52, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 31: 100%|‚ñà| 13/13 [00:12<00:00,  1.01it/s, loss=2.52, v_num=ihla, train_lo\u001b[AEpoch 31, global step 351: NN was not in top 3\n",
      "Epoch 32:  92%|‚ñâ| 12/13 [00:11<00:00,  1.05it/s, loss=2.5, v_num=ihla, train_los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.51s/it]\u001b[A\n",
      "Epoch 32: 100%|‚ñà| 13/13 [00:13<00:00,  1.03s/it, loss=2.5, v_num=ihla, train_los\u001b[A\n",
      "Epoch 32: 100%|‚ñà| 13/13 [00:13<00:00,  1.03s/it, loss=2.5, v_num=ihla, train_losEpoch 32, global step 362: NN was not in top 3\n",
      "Epoch 33:  92%|‚ñâ| 12/13 [00:10<00:00,  1.10it/s, loss=2.6, v_num=ihla, train_los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.55s/it]\u001b[A\n",
      "Epoch 33: 100%|‚ñà| 13/13 [00:12<00:00,  1.01it/s, loss=2.6, v_num=ihla, train_los\u001b[A\n",
      "Epoch 33: 100%|‚ñà| 13/13 [00:12<00:00,  1.01it/s, loss=2.6, v_num=ihla, train_losEpoch 33, global step 373: NN was not in top 3\n",
      "Epoch 34:  92%|‚ñâ| 12/13 [00:10<00:00,  1.17it/s, loss=2.53, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.52s/it]\u001b[A\n",
      "Epoch 34: 100%|‚ñà| 13/13 [00:12<00:00,  1.07it/s, loss=2.53, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 34: 100%|‚ñà| 13/13 [00:12<00:00,  1.07it/s, loss=2.53, v_num=ihla, train_loEpoch 34, global step 384: NN was not in top 3\n",
      "Epoch 35:  92%|‚ñâ| 12/13 [00:10<00:00,  1.13it/s, loss=2.51, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.55s/it]\u001b[A\n",
      "Epoch 35: 100%|‚ñà| 13/13 [00:12<00:00,  1.03it/s, loss=2.51, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 35: 100%|‚ñà| 13/13 [00:12<00:00,  1.03it/s, loss=2.51, v_num=ihla, train_loEpoch 35, global step 395: NN was not in top 3\n",
      "Epoch 36:  92%|‚ñâ| 12/13 [00:10<00:00,  1.15it/s, loss=2.42, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.55s/it]\u001b[A\n",
      "Epoch 36: 100%|‚ñà| 13/13 [00:12<00:00,  1.05it/s, loss=2.42, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 36: 100%|‚ñà| 13/13 [00:12<00:00,  1.05it/s, loss=2.42, v_num=ihla, train_loEpoch 36, global step 406: NN was not in top 3\n",
      "Epoch 37:  92%|‚ñâ| 12/13 [00:09<00:00,  1.24it/s, loss=2.41, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.61s/it]\u001b[A\n",
      "Epoch 37: 100%|‚ñà| 13/13 [00:11<00:00,  1.11it/s, loss=2.41, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 37: 100%|‚ñà| 13/13 [00:11<00:00,  1.11it/s, loss=2.41, v_num=ihla, train_loEpoch 37, global step 417: NN reached 0.32500 (best 0.32500), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=37-NN=0.3250-mAP=0.5434-train_loss=2.3771-val_loss=1.8695.ckpt\" as top 3\n",
      "Epoch 38:  92%|‚ñâ| 12/13 [00:09<00:00,  1.23it/s, loss=2.39, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.60s/it]\u001b[A\n",
      "Epoch 38: 100%|‚ñà| 13/13 [00:11<00:00,  1.11it/s, loss=2.39, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 38: 100%|‚ñà| 13/13 [00:11<00:00,  1.11it/s, loss=2.39, v_num=ihla, train_lo\u001b[AEpoch 38, global step 428: NN was not in top 3\n",
      "Epoch 39:  92%|‚ñâ| 12/13 [00:10<00:00,  1.12it/s, loss=2.36, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.54s/it]\u001b[A\n",
      "Epoch 39: 100%|‚ñà| 13/13 [00:12<00:00,  1.02it/s, loss=2.36, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 39: 100%|‚ñà| 13/13 [00:12<00:00,  1.02it/s, loss=2.36, v_num=ihla, train_loEpoch 39, global step 439: NN reached 0.27500 (best 0.32500), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=39-NN=0.2750-mAP=0.4790-train_loss=2.4955-val_loss=2.0959.ckpt\" as top 3\n",
      "Epoch 40:  92%|‚ñâ| 12/13 [00:10<00:00,  1.11it/s, loss=2.4, v_num=ihla, train_los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.60s/it]\u001b[A\n",
      "Epoch 40: 100%|‚ñà| 13/13 [00:12<00:00,  1.02it/s, loss=2.4, v_num=ihla, train_los\u001b[A\n",
      "Epoch 40: 100%|‚ñà| 13/13 [00:12<00:00,  1.01it/s, loss=2.4, v_num=ihla, train_losEpoch 40, global step 450: NN reached 0.32500 (best 0.32500), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=40-NN=0.3250-mAP=0.5453-train_loss=2.3706-val_loss=1.8161.ckpt\" as top 3\n",
      "Epoch 41:  92%|‚ñâ| 12/13 [00:09<00:00,  1.29it/s, loss=2.34, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.67s/it]\u001b[A\n",
      "Epoch 41: 100%|‚ñà| 13/13 [00:11<00:00,  1.14it/s, loss=2.34, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 41: 100%|‚ñà| 13/13 [00:11<00:00,  1.14it/s, loss=2.34, v_num=ihla, train_loEpoch 41, global step 461: NN reached 0.32500 (best 0.32500), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=41-NN=0.3250-mAP=0.5131-train_loss=2.3990-val_loss=1.9765.ckpt\" as top 3\n",
      "Epoch 42:  92%|‚ñâ| 12/13 [00:10<00:00,  1.19it/s, loss=2.34, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.62s/it]\u001b[A\n",
      "Epoch 42: 100%|‚ñà| 13/13 [00:12<00:00,  1.07it/s, loss=2.34, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 42: 100%|‚ñà| 13/13 [00:12<00:00,  1.07it/s, loss=2.34, v_num=ihla, train_loEpoch 42, global step 472: NN reached 0.32500 (best 0.32500), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=42-NN=0.3250-mAP=0.5109-train_loss=2.3816-val_loss=1.8919.ckpt\" as top 3\n",
      "Epoch 43:  92%|‚ñâ| 12/13 [00:15<00:01,  1.32s/it, loss=2.28, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.70s/it]\u001b[A\n",
      "Epoch 43: 100%|‚ñà| 13/13 [00:19<00:00,  1.53s/it, loss=2.28, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 43: 100%|‚ñà| 13/13 [00:19<00:00,  1.54s/it, loss=2.28, v_num=ihla, train_loEpoch 43, global step 483: NN was not in top 3\n",
      "Epoch 44:  92%|‚ñâ| 12/13 [00:20<00:01,  1.69s/it, loss=2.22, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.82s/it]\u001b[A\n",
      "Epoch 44: 100%|‚ñà| 13/13 [00:22<00:00,  1.73s/it, loss=2.22, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 44: 100%|‚ñà| 13/13 [00:22<00:00,  1.73s/it, loss=2.22, v_num=ihla, train_loEpoch 44, global step 494: NN was not in top 3\n",
      "Epoch 45:  92%|‚ñâ| 12/13 [00:17<00:01,  1.46s/it, loss=2.18, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.74s/it]\u001b[A\n",
      "Epoch 45: 100%|‚ñà| 13/13 [00:22<00:00,  1.73s/it, loss=2.18, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 45: 100%|‚ñà| 13/13 [00:22<00:00,  1.73s/it, loss=2.18, v_num=ihla, train_lo\u001b[AEpoch 45, global step 505: NN reached 0.45000 (best 0.45000), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=45-NN=0.4500-mAP=0.6102-train_loss=2.2762-val_loss=1.7897.ckpt\" as top 3\n",
      "Epoch 46:  92%|‚ñâ| 12/13 [00:17<00:01,  1.45s/it, loss=2.19, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.66s/it]\u001b[A\n",
      "Epoch 46: 100%|‚ñà| 13/13 [00:23<00:00,  1.78s/it, loss=2.19, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 46: 100%|‚ñà| 13/13 [00:23<00:00,  1.78s/it, loss=2.19, v_num=ihla, train_loEpoch 46, global step 516: NN reached 0.35000 (best 0.45000), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=46-NN=0.3500-mAP=0.5515-train_loss=2.1947-val_loss=1.8604.ckpt\" as top 3\n",
      "Epoch 47:  92%|‚ñâ| 12/13 [00:15<00:01,  1.33s/it, loss=2.1, v_num=ihla, train_los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.86s/it]\u001b[A\n",
      "Epoch 47: 100%|‚ñà| 13/13 [00:18<00:00,  1.41s/it, loss=2.1, v_num=ihla, train_los\u001b[A\n",
      "Epoch 47: 100%|‚ñà| 13/13 [00:18<00:00,  1.41s/it, loss=2.1, v_num=ihla, train_losEpoch 47, global step 527: NN reached 0.42500 (best 0.45000), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=47-NN=0.4250-mAP=0.5900-train_loss=2.0402-val_loss=1.7486.ckpt\" as top 3\n",
      "Epoch 48:  92%|‚ñâ| 12/13 [00:15<00:01,  1.26s/it, loss=2.02, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.86s/it]\u001b[A\n",
      "Epoch 48: 100%|‚ñà| 13/13 [00:20<00:00,  1.57s/it, loss=2.02, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 48: 100%|‚ñà| 13/13 [00:20<00:00,  1.58s/it, loss=2.02, v_num=ihla, train_lo\u001b[AEpoch 48, global step 538: NN reached 0.47500 (best 0.47500), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=48-NN=0.4750-mAP=0.6278-train_loss=1.9991-val_loss=1.7449.ckpt\" as top 3\n",
      "Epoch 49:  92%|‚ñâ| 12/13 [00:15<00:01,  1.27s/it, loss=2.04, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.83s/it]\u001b[A\n",
      "Epoch 49: 100%|‚ñà| 13/13 [00:17<00:00,  1.35s/it, loss=2.04, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 49: 100%|‚ñà| 13/13 [00:17<00:00,  1.35s/it, loss=2.04, v_num=ihla, train_loEpoch 49, global step 549: NN was not in top 3\n",
      "Epoch 50:  92%|‚ñâ| 12/13 [00:15<00:01,  1.26s/it, loss=2.06, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.92s/it]\u001b[A\n",
      "Epoch 50: 100%|‚ñà| 13/13 [00:17<00:00,  1.37s/it, loss=2.06, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 50: 100%|‚ñà| 13/13 [00:17<00:00,  1.37s/it, loss=2.06, v_num=ihla, train_loEpoch 50, global step 560: NN was not in top 3\n",
      "Epoch 51:  92%|‚ñâ| 12/13 [00:17<00:01,  1.49s/it, loss=2.02, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.87s/it]\u001b[A\n",
      "Epoch 51: 100%|‚ñà| 13/13 [00:20<00:00,  1.55s/it, loss=2.02, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 51: 100%|‚ñà| 13/13 [00:20<00:00,  1.56s/it, loss=2.02, v_num=ihla, train_loEpoch 51, global step 571: NN was not in top 3\n",
      "Epoch 52:  92%|‚ñâ| 12/13 [00:15<00:01,  1.26s/it, loss=2.02, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.98s/it]\u001b[A\n",
      "Epoch 52: 100%|‚ñà| 13/13 [00:18<00:00,  1.43s/it, loss=2.02, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 52: 100%|‚ñà| 13/13 [00:18<00:00,  1.43s/it, loss=2.02, v_num=ihla, train_lo\u001b[AEpoch 52, global step 582: NN was not in top 3\n",
      "Epoch 53:  92%|‚ñâ| 12/13 [00:17<00:01,  1.48s/it, loss=2.01, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:02<00:02,  2.05s/it]\u001b[A\n",
      "Epoch 53: 100%|‚ñà| 13/13 [00:20<00:00,  1.56s/it, loss=2.01, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 53: 100%|‚ñà| 13/13 [00:20<00:00,  1.56s/it, loss=2.01, v_num=ihla, train_loEpoch 53, global step 593: NN was not in top 3\n",
      "Epoch 54:  92%|‚ñâ| 12/13 [00:14<00:01,  1.19s/it, loss=1.86, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.76s/it]\u001b[A\n",
      "Epoch 54: 100%|‚ñà| 13/13 [00:18<00:00,  1.43s/it, loss=1.86, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 54: 100%|‚ñà| 13/13 [00:18<00:00,  1.43s/it, loss=1.86, v_num=ihla, train_loEpoch 54, global step 604: NN reached 0.45000 (best 0.47500), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=54-NN=0.4500-mAP=0.6229-train_loss=1.7903-val_loss=1.6601.ckpt\" as top 3\n",
      "Epoch 55:  92%|‚ñâ| 12/13 [00:14<00:01,  1.22s/it, loss=1.92, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:02<00:02,  2.89s/it]\u001b[A\n",
      "Epoch 55: 100%|‚ñà| 13/13 [00:17<00:00,  1.38s/it, loss=1.92, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 55: 100%|‚ñà| 13/13 [00:17<00:00,  1.38s/it, loss=1.92, v_num=ihla, train_loEpoch 55, global step 615: NN reached 0.55000 (best 0.55000), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=55-NN=0.5500-mAP=0.7200-train_loss=1.9752-val_loss=1.4056.ckpt\" as top 3\n",
      "Epoch 56:  92%|‚ñâ| 12/13 [00:14<00:01,  1.23s/it, loss=1.77, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.83s/it]\u001b[A\n",
      "Epoch 56: 100%|‚ñà| 13/13 [00:16<00:00,  1.30s/it, loss=1.77, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 56: 100%|‚ñà| 13/13 [00:16<00:00,  1.30s/it, loss=1.77, v_num=ihla, train_loEpoch 56, global step 626: NN reached 0.52500 (best 0.55000), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=56-NN=0.5250-mAP=0.7027-train_loss=1.6603-val_loss=1.3866.ckpt\" as top 3\n",
      "Epoch 57:  92%|‚ñâ| 12/13 [00:17<00:01,  1.45s/it, loss=1.65, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.79s/it]\u001b[A\n",
      "Epoch 57: 100%|‚ñà| 13/13 [00:21<00:00,  1.63s/it, loss=1.65, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 57: 100%|‚ñà| 13/13 [00:21<00:00,  1.63s/it, loss=1.65, v_num=ihla, train_loEpoch 57, global step 637: NN reached 0.65000 (best 0.65000), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=57-NN=0.6500-mAP=0.7925-train_loss=1.6123-val_loss=1.2212.ckpt\" as top 3\n",
      "Epoch 58:  92%|‚ñâ| 12/13 [00:17<00:01,  1.48s/it, loss=1.6, v_num=ihla, train_los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.99s/it]\u001b[A\n",
      "Epoch 58: 100%|‚ñà| 13/13 [00:22<00:00,  1.71s/it, loss=1.6, v_num=ihla, train_los\u001b[A\n",
      "Epoch 58: 100%|‚ñà| 13/13 [00:22<00:00,  1.71s/it, loss=1.6, v_num=ihla, train_losEpoch 58, global step 648: NN reached 0.62500 (best 0.65000), saving model to \"./runs/hcmus-shrec23-textANIMAR/l490ihla/checkpoints/baseline-epoch=58-NN=0.6250-mAP=0.7591-train_loss=1.5900-val_loss=1.2554.ckpt\" as top 3\n",
      "Epoch 59:  92%|‚ñâ| 12/13 [00:17<00:01,  1.49s/it, loss=1.42, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.80s/it]\u001b[A\n",
      "Epoch 59: 100%|‚ñà| 13/13 [00:20<00:00,  1.61s/it, loss=1.42, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 59: 100%|‚ñà| 13/13 [00:20<00:00,  1.61s/it, loss=1.42, v_num=ihla, train_lo\u001b[AEpoch 59, global step 659: NN was not in top 3\n",
      "Epoch 60:  92%|‚ñâ| 12/13 [00:19<00:01,  1.60s/it, loss=1.42, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.86s/it]\u001b[A\n",
      "Epoch 60: 100%|‚ñà| 13/13 [00:22<00:00,  1.70s/it, loss=1.42, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 60: 100%|‚ñà| 13/13 [00:22<00:00,  1.70s/it, loss=1.42, v_num=ihla, train_loEpoch 60, global step 670: NN was not in top 3\n",
      "Epoch 61:  92%|‚ñâ| 12/13 [00:15<00:01,  1.30s/it, loss=1.52, v_num=ihla, train_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1/2 [00:01<00:01,  1.82s/it]\u001b[A\n",
      "Epoch 61: 100%|‚ñà| 13/13 [00:17<00:00,  1.37s/it, loss=1.52, v_num=ihla, train_lo\u001b[A\n",
      "Epoch 61: 100%|‚ñà| 13/13 [00:17<00:00,  1.38s/it, loss=1.52, v_num=ihla, train_loEpoch 61, global step 681: NN was not in top 3\n",
      "Epoch 62:   0%| | 0/13 [00:00<?, ?it/s, loss=1.52, v_num=ihla, train_loss_step=1"
     ]
    }
   ],
   "source": [
    "%cd $WORKDIR\n",
    "!python src/train.py -c configs/baseline.yml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
